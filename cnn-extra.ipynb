{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tce3stUlHN0L"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "tuOe1ymfHZPu"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MfBg1C5NB3X0"
   },
   "source": [
    "# Entrenamiento multitrabajador con Keras\n",
    "\n",
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/distribute/multi_worker_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
    "  <td>     <a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/distribute/multi_worker_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver código fuente en GitHub</a>\n",
    "</td>\n",
    "  <td>     <a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/distribute/multi_worker_with_keras.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar bloc de notas</a>\n",
    "</td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHxb-dlhMIzW"
   },
   "source": [
    "## Visión general\n",
    "\n",
    "Este tutorial demuestra cómo realizar un entrenamiento distribuido multitrabajador con un modelo Keras y la API `Model.fit` usando la API `tf.distribute.MultiWorkerMirroredStrategy`. Con la ayuda de esta estrategia, un modelo Keras que ha sido diseñado para ejecutarse en un único trabajador puede funcionar sin problemas en multitrabajador con cambios mínimos en el código.\n",
    "\n",
    "Para aprender a usar la `MultiWorkerMirroredStrategy` con Keras y un bucle de entrenamiento personalizado, consulte [Bucle de entrenamiento personalizado con Keras y MultiWorkerMirroredStrategy](multi_worker_with_ctl.ipynb).\n",
    "\n",
    "Este tutorial incluye un ejemplo mínimo de multitrabajador con dos trabajadores a modo de demostración."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JUdRerXg6yz3"
   },
   "source": [
    "### Elegir la estrategia adecuada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAiCV_oL63GM"
   },
   "source": [
    "Antes de empezar, cerciórese de que `tf.distribute.MultiWorkerMirroredStrategy` es la opción adecuada para su(s) acelerador(es) y entrenamiento. Se trata de dos formas habituales de distribuir el entrenamiento con paralelismo de datos:\n",
    "\n",
    "- *Entrenamiento síncrono*, en el que los pasos del entrenamiento se sincronizan entre los trabajadores y las réplicas, como `tf.distribute.MirroredStrategy`, `tf.distribute.TPUStrategy`, y `tf.distribute.MultiWorkerMirroredStrategy`. Todos los trabajadores se entrenan sobre diferentes porciones de datos de entrada de forma sincronizada, y agregando gradientes en cada paso.\n",
    "- *Entrenamiento asíncrono*, en el que los pasos del entrenamiento no están estrictamente sincronizados, como `tf.distribute.experimental.ParameterServerStrategy`. Todos los trabajadores están entrenando de forma independiente sobre los datos de entrada y actualizando las variables de forma asíncrona.\n",
    "\n",
    "Si lo que busca es un entrenamiento síncrono multitrabajador sin TPU, entonces necesita `tf.distribute.MultiWorkerMirroredStrategy`. Crea copias de todas las variables de las capas del modelo en cada dispositivo a través de todos los trabajadores. Usa `CollectiveOps`, una op de TensorFlow para la comunicación colectiva, para agregar gradientes y conserva las variables sincronizadas. Si está interesado, consulte la información sobre el parámetro `tf.distribute.experimental.CommunicationOptions` para ver las opciones de implementación de colectivos.\n",
    "\n",
    "Consulte [Entrenamiento distribuido en TensorFlow](../../guide/distributed_training.ipynb) para ver un resumen de las API `tf.distribute.Strategy`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUXex9ctTuDB"
   },
   "source": [
    "## Preparación\n",
    "\n",
    "Comience con algunas importaciones necesarias:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnYxvfLD-LW-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz0EY91y3mxy"
   },
   "source": [
    "Antes de importar TensorFlow, realice algunos cambios en el entorno:\n",
    "\n",
    "- En una aplicación del mundo real, cada trabajador estaría en una máquina diferente. Para efectos de este tutorial, todos los trabajadores se ejecutarán en la máquina **this**. Así pues, desactive todas las GPU para evitar errores causados por todos los trabajadores que intentan usar la misma GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpEIVI5upIzM"
   },
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7X1MS6385BWi"
   },
   "source": [
    "- Restablezca la variable de entorno `TF_CONFIG` (aprenderá más sobre esto más adelante):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEJLYa2_7OZF"
   },
   "outputs": [],
   "source": [
    "os.environ.pop('TF_CONFIG', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rd4L9Ii77SS8"
   },
   "source": [
    "- Asegúrese de que el directorio actual está en la ruta de Python; esto permite al bloc de notas importar posteriormente los archivos escritos por `%%writefile`:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPBuZUNSZmrQ"
   },
   "outputs": [],
   "source": [
    "if '.' not in sys.path:\n",
    "  sys.path.insert(0, '.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9hLpDZhAz2q-"
   },
   "source": [
    "Instale `tf-nightly`, ya que la frecuencia de guardado de puntos de verificación en un paso concreto con el argumento `save_freq` en `tf.keras.callbacks.BackupAndRestore` se introduce a partir de TensorFlow 2.10:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-XqozLfzz30N"
   },
   "outputs": [],
   "source": [
    "!pip install tf-nightly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "524e38dab658"
   },
   "source": [
    "Finalmente, importe TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vHNvttzV43sA"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0S2jpf6Sx50i"
   },
   "source": [
    "### Definición del conjunto de datos y del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fLW6D2TzvC-4"
   },
   "source": [
    "A continuación, cree un archivo `mnist_setup.py` con una configuración sencilla del modelo y el conjunto de datos. Este archivo Python será usado por los procesos de los trabajadores en este tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dma_wUAxZqo2"
   },
   "outputs": [],
   "source": [
    "%%writefile mnist_setup.py\n",
    "\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "def mnist_dataset(batch_size):\n",
    "  (x_train, y_train), _ = tf.keras.datasets.mnist.load_data()\n",
    "  # The `x` arrays are in uint8 and have values in the [0, 255] range.\n",
    "  # You need to convert them to float32 with values in the [0, 1] range.\n",
    "  x_train = x_train / np.float32(255)\n",
    "  y_train = y_train.astype(np.int64)\n",
    "  train_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "      (x_train, y_train)).shuffle(60000).repeat().batch(batch_size)\n",
    "  return train_dataset\n",
    "\n",
    "def build_and_compile_cnn_model():\n",
    "  model = tf.keras.Sequential([\n",
    "      tf.keras.layers.InputLayer(input_shape=(28, 28)),\n",
    "      tf.keras.layers.Reshape(target_shape=(28, 28, 1)),\n",
    "      tf.keras.layers.Conv2D(32, 3, activation='relu'),\n",
    "      tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(128, activation='relu'),\n",
    "      tf.keras.layers.Dense(10)\n",
    "  ])\n",
    "  model.compile(\n",
    "      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "      optimizer=tf.keras.optimizers.SGD(learning_rate=0.001),\n",
    "      metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UL3kisMO90X"
   },
   "source": [
    "### Entrenamiento de modelo en un solo trabajador\n",
    "\n",
    "Pruebe a entrenar el modelo durante un pequeño número de épocas y observe los resultados de *un solo trabajador* para asegurarse de que todo funciona correctamente. Conforme avance el entrenamiento, la pérdida debería disminuir y la precisión debería aumentar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Qe6iAf5O8iJ"
   },
   "outputs": [],
   "source": [
    "import mnist_setup\n",
    "\n",
    "batch_size = 64\n",
    "single_worker_dataset = mnist_setup.mnist_dataset(batch_size)\n",
    "single_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "single_worker_model.fit(single_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JmgZwwymxqt5"
   },
   "source": [
    "## Configuración multitrabajador\n",
    "\n",
    "Entremos ahora en el mundo del entrenamiento multitrabajador.\n",
    "\n",
    "### Un cluster con trabajos y tareas\n",
    "\n",
    "En TensorFlow, el entrenamiento distribuido implica un `'cluster'` con varios trabajos, y cada uno de los trabajos puede tener una o más `'task'`.\n",
    "\n",
    "Necesitará la variable de entorno de configuración `TF_CONFIG` para el entrenamiento en múltiples máquinas, cada una de las cuales posiblemente tenga un rol diferente. `TF_CONFIG` es una cadena JSON empleada para especificar la configuración del clúster para cada trabajador que forme parte del clúster.\n",
    "\n",
    "Hay dos componentes de una variable `TF_CONFIG`: `'cluster'` y `'task'`.\n",
    "\n",
    "- Un `'cluster'` es el mismo para todos los trabajadores y da información sobre el cluster de entrenamiento, que es un diccionario formado por diferentes tipos de trabajos, como `'worker'` o `'chief'`.\n",
    "\n",
    "    - En el entrenamiento multitrabajador con `tf.distribute.MultiWorkerMirroredStrategy`, suele haber un `'worker'` que asume más responsabilidades, como guardar un punto de verificación y escribir un archivo sumario para TensorBoard, además de lo que hace un `'worker'` normal. A este `'worker'` se le identifica como el \"worker\" jefe (con un nombre de trabajo `'chief'`).\n",
    "    - Es habitual que el trabajador con `'index'` `0` sea el `'chief'`.\n",
    "\n",
    "- Una `'task'` da información sobre la tarea actual y es diferente para cada trabajador. Especifica el `'type'` y el `'index'` de ese trabajador.\n",
    "\n",
    "Below is an example configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XK1eTYvSZiX7"
   },
   "outputs": [],
   "source": [
    "tf_config = {\n",
    "    'cluster': {\n",
    "        'worker': ['localhost:12345', 'localhost:23456']\n",
    "    },\n",
    "    'task': {'type': 'worker', 'index': 0}\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjgwJbPKZkJL"
   },
   "source": [
    "Tenga en cuenta que `tf_config` es sólo una variable local en Python. Para usarla para la configuración del entrenamiento, serialícela como JSON y colóquela en una variable de entorno `TF_CONFIG`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yY-T0YDQZjbu"
   },
   "outputs": [],
   "source": [
    "json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8YFpxrcsZ2xG"
   },
   "source": [
    "En el ejemplo de configuración anterior, se establece el `'type'` de tarea a `'worker'` y el `'index'` de la tarea en `0`. Por lo tanto, esta máquina es la *primera* trabajadora. Será designada como el trabajador `'chief'`.\n",
    "\n",
    "Nota: Otras máquinas necesitarán tener la variable de entorno `TF_CONFIG` establecida también, y deberá tener el mismo diccionario `'cluster'`, pero diferentes `'type'` de tareas o `'index'` de tareas, dependiendo de los roles de esas máquinas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aogb74kHxynz"
   },
   "source": [
    "En la práctica, usted crearía varios trabajadores en direcciones IP/puertos externos y configuraría una variable `TF_CONFIG` en cada trabajador según corresponda. Como ejemplo, este tutorial muestra cómo puede configurar una variable `TF_CONFIG` con dos trabajadores en un `localhost`:\n",
    "\n",
    "- El `TF_CONFIG` del primer trabajador (`'chief'`) como se muestra arriba.\n",
    "- Para el segundo trabajador, fijará `tf_config['task']['index']=1`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIlkfWmjz1PG"
   },
   "source": [
    "### Variables de entorno y subprocesos en blocs de notas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FcjAbuGY1ACJ"
   },
   "source": [
    "Los subprocesos heredan las variables de entorno de su padre. Así que si configura una variable de entorno en este proceso de bloc de notas Jupyter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PH2gHn2_0_U8"
   },
   "outputs": [],
   "source": [
    "os.environ['GREETINGS'] = 'Hello TensorFlow!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gQkIX-cg18md"
   },
   "source": [
    "... entonces puede acceder a la variable de entorno desde los subprocesos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pquKO6IA18G5"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo ${GREETINGS}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "af6BCA-Y2fpz"
   },
   "source": [
    "En la siguiente sección, usará este método para pasar el `TF_CONFIG` a los subprocesos de los trabajadores. En un escenario real nunca ejecutaría sus trabajos de esta forma; este tutorial sólo muestra cómo hacerlo con un ejemplo multitrabajador mínimo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnDJmaRA9qnf"
   },
   "source": [
    "## Entrenar el modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhNtHfuxCGVy"
   },
   "source": [
    "Para entrenar el modelo, cree en primer lugar una instancia del modelo `tf.distribute.MultiWorkerMirroredStrategy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uFSHCJXMrQ-"
   },
   "outputs": [],
   "source": [
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0iv7SyyAohc"
   },
   "source": [
    "Nota: `TF_CONFIG` se parsea y los servidores GRPC de TensorFlow se inician en el momento en que se llama a `MultiWorkerMirroredStrategy`, por lo que la variable de entorno `TF_CONFIG` debe configurarse antes de crear una instancia `tf.distribute.Strategy`. Dado que `TF_CONFIG` aún no está establecida, la estrategia anterior es efectivamente un entrenamiento para un solo trabajador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H47DDcOgfzm7"
   },
   "source": [
    "Con la integración de la API `tf.distribute.Strategy` en `tf.keras`, el único cambio que hará para distribuir el entrenamiento a múltiples trabajadores es acotar la construcción del modelo y la llamada `model.compile` dentro de `strategy.scope()`. El ámbito de la estrategia de distribución dicta cómo y dónde se crean las variables, y en el caso de `MultiWorkerMirroredStrategy`, las variables creadas son `MirroredVariable`, y se replican en cada uno de los trabajadores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wo6b9wX65glL"
   },
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "  # Model building/compiling need to be within `strategy.scope()`.\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhq3fzyR5hTw"
   },
   "source": [
    "Nota: Actualmente existe una limitación en `MultiWorkerMirroredStrategy` en la que las operaciones TensorFlow deben crearse después de crear la instancia de la estrategia. Si se encuentra con el error `RuntimeError: Collective ops must be configured at program startup`, pruebe a crear la instancia de `MultiWorkerMirroredStrategy` al principio del programa y ponga el código que pueda crear ops después de instanciar la estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jfYpmIxO6Jck"
   },
   "source": [
    "Para ejecutar realmente con `MultiWorkerMirroredStrategy` necesitará ejecutar procesos worker y pasarles un `TF_CONFIG`.\n",
    "\n",
    "Al igual que el archivo `mnist_setup.py` escrito anteriormente, aquí está el `main.py` que cada trabajador ejecutará:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BcsuBYrpgnlS"
   },
   "outputs": [],
   "source": [
    "%%writefile main.py\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "import tensorflow as tf\n",
    "import mnist_setup\n",
    "\n",
    "per_worker_batch_size = 64\n",
    "tf_config = json.loads(os.environ['TF_CONFIG'])\n",
    "num_workers = len(tf_config['cluster']['worker'])\n",
    "\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "\n",
    "global_batch_size = per_worker_batch_size * num_workers\n",
    "multi_worker_dataset = mnist_setup.mnist_dataset(global_batch_size)\n",
    "\n",
    "with strategy.scope():\n",
    "  # Model building/compiling need to be within `strategy.scope()`.\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "\n",
    "\n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=3, steps_per_epoch=70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aom9xelvJQ_6"
   },
   "source": [
    "En el fragmento de código anterior, observe que `global_batch_size`, que se pasa a `Dataset.batch`, se configura en `per_worker_batch_size * num_workers`. Esto garantiza que cada trabajador procese lotes de `per_worker_batch_size` ejemplos independientemente del número de trabajadores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lHLhOii67Saa"
   },
   "source": [
    "El directorio actual contiene ahora ambos archivos Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bi6x05Sr60O9"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "ls *.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmEEStPS6vR_"
   },
   "source": [
    "Serialice el `TF_CONFIG` a JSON y añádalo a las variables de entorno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9uu3g7vV7Bbt"
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsY3dQLK7jdf"
   },
   "source": [
    "Ahora, puede iniciar un proceso trabajador que ejecutará el `main.py` y usará el `TF_CONFIG`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txMXaq8d8N_S"
   },
   "outputs": [],
   "source": [
    "# first kill any previous runs\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qnSma_Ck7r-r"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "python main.py &> job_0.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZChyazqS7v0P"
   },
   "source": [
    "Hay que tener en cuenta algunas cosas sobre el comando anterior:\n",
    "\n",
    "1. Éste usa el `%%bash` que es una [\"magia\" de bloc de notas](https://ipython.readthedocs.io/en/stable/interactive/magics.html) para ejecutar algunos comandos bash.\n",
    "2. Usa la bandera `--bg` para ejecutar el proceso `bash` en segundo plano, porque este trabajador no terminará. Espera a todos los trabajadores antes de iniciarse.\n",
    "\n",
    "El proceso trabajador en segundo plano no imprimirá la salida en este bloc de notas, por lo que el `&>` redirige su salida a un archivo para que pueda inspeccionar lo sucedido en un archivo de registro más adelante.\n",
    "\n",
    "Por lo tanto, espere unos segundos a que se inicie el proceso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hm2yrULE9281"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZFPoNxg_9_Mx"
   },
   "source": [
    "Ahora, inspeccione lo que se ha volcado en el archivo de registro del trabajador hasta el momento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vZEOuVgQ9-hn"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat job_0.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RqZhVF7L_KOy"
   },
   "source": [
    "La última línea del archivo de registro debería decir: `Started server with target: grpc://localhost:12345`. El primer trabajador ya está listo y está esperando a que todos los demás trabajadores estén listos para continuar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pi8vPNNA_l4a"
   },
   "source": [
    "Así que actualice el `tf_config` para que el proceso del segundo trabajador retome el trabajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lAiYkkPu_Jqd"
   },
   "outputs": [],
   "source": [
    "tf_config['task']['index'] = 1\n",
    "os.environ['TF_CONFIG'] = json.dumps(tf_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0AshGVO0_x0w"
   },
   "source": [
    "Inicie el segundo trabajador. Esto iniciará el entrenamiento, ya que todos los trabajadores están activos (por lo que no es necesario poner este proceso en segundo plano):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ESVtyQ9_xjx"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hX4FA2O2AuAn"
   },
   "source": [
    "Si se vuelven a revisar los registros escritos por el primer trabajador, se sabrá que participó en el entrenamiento de ese modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rc6hw3yTBKXX"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cat job_0.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zL79ak5PMzEg"
   },
   "source": [
    "Nota: Esto puede correr más lento que la ejecución de prueba al principio de este tutorial porque ejecutar múltiples trabajadores en una sola máquina sólo añade sobrecarga. La meta aquí no es mejorar el tiempo de entrenamiento sino dar un ejemplo de entrenamiento multitrabajador.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sG5_1UgrgniF"
   },
   "outputs": [],
   "source": [
    "# Delete the `TF_CONFIG`, and kill any background tasks so they don't affect the next section.\n",
    "os.environ.pop('TF_CONFIG', None)\n",
    "%killbgscripts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j2FJVHoUIrE"
   },
   "source": [
    "## Entrenamiento multitrabajador a fondo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C1hBks_dAZmT"
   },
   "source": [
    "Por el momento, ha aprendido a realizar una configuración multitrabajador básica. El resto del tutorial repasa en detalle otros factores que pueden ser útiles o importantes para casos de uso reales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rr14Vl9GR4zq"
   },
   "source": [
    "### Fragmentación de conjuntos de datos\n",
    "\n",
    "En el entrenamiento multitrabajador, *es necesario fragmentar el conjunto de datos* para garantizar la convergencia y el rendimiento.\n",
    "\n",
    "El ejemplo de la sección anterior se basa en el autosharding predeterminado proporcionado por la API `tf.distribute.Strategy`. Puede controlar el fragmentado estableciendo la `tf.data.experimental.AutoShardPolicy` del `tf.data.experimental.DistributeOptions`.\n",
    "\n",
    "Para saber más sobre *auto fragmentación*, consulte la [Guía de entrada distribuida](https://www.tensorflow.org/tutorials/distribute/input#sharding).\n",
    "\n",
    "Aquí tiene un ejemplo rápido de cómo desactivar la auto fragmentación, para que cada réplica procese cada ejemplo (*no recomendado*):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxEtdh1vH-TF"
   },
   "outputs": [],
   "source": [
    "options = tf.data.Options()\n",
    "options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.OFF\n",
    "\n",
    "global_batch_size = 64\n",
    "multi_worker_dataset = mnist_setup.mnist_dataset(batch_size=64)\n",
    "dataset_no_auto_shard = multi_worker_dataset.with_options(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z85hElxsBQsT"
   },
   "source": [
    "### Evaluación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmqvlh5LhAoU"
   },
   "source": [
    "Si pasa también el `validation_data` a `Model.fit`, alternará entre entrenamiento y evaluación para cada época. El trabajo de evaluación se distribuye entre el mismo grupo de trabajadores, y sus resultados se agregan y están disponibles para todos los trabajadores.\n",
    "\n",
    "Parecido al entrenamiento, el conjunto de datos de validación se auto fragmenta a nivel de archivo. Debe configurar un tamaño de lote global en el conjunto de datos de validación y configurar los `validation_steps`.\n",
    "\n",
    "Se recomienda utilizar un conjunto de datos repetido (llamando a `tf.data.Dataset.repeat`) para la evaluación.\n",
    "\n",
    "Otra posibilidad es crear otra tarea que lea periódicamente los puntos de verificación y ejecute la evaluación. Esto es lo que hace un Estimator. Pero no se recomienda esta forma de realizar la evaluación, por lo que se omiten sus detalles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNkoxUPJBNTb"
   },
   "source": [
    "### Rendimiento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVk4ftYx6JAO"
   },
   "source": [
    "Para mejorar el rendimiento del entrenamiento multitrabajador, puede probar lo siguiente:\n",
    "\n",
    "- `tf.distribute.MultiWorkerMirroredStrategy` provee múltiples [implementaciones de comunicación de colectivos](https://www.tensorflow.org/api_docs/python/tf/distribute/experimental/CommunicationImplementation):\n",
    "\n",
    "    - `RING` implementa colectivos en anillos usando gRPC como capa de comunicación entre hosts.\n",
    "    - `NCCL` usa la [Librería de Comunicación Colectiva de NVIDIA](https://developer.nvidia.com/nccl) para implementar los colectivos.\n",
    "    - `AUTO` deja la elección al runtime.\n",
    "\n",
    "    La mejor elección para implementar colectivos depende del número y el tipo de GPU y de las interconexiones de red del cluster. Para anular la elección automática, especifique el parámetro `communication_options` del constructor de `MultiWorkerMirroredStrategy`. Por ejemplo:\n",
    "\n",
    "    ```python\n",
    "    communication_options=tf.distribute.experimental.CommunicationOptions(implementation=tf.distribute.experimental.CommunicationImplementation.NCCL)\n",
    "    ```\n",
    "\n",
    "- Haga cast de las variables a `tf.float` si es posible:\n",
    "\n",
    "    - El modelo oficial de ResNet incluye [un ejemplo](https://github.com/tensorflow/models/blob/8367cf6dabe11adf7628541706b660821f397dce/official/resnet/resnet_model.py#L466) de cómo hacerlo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97WhAu8uKw3j"
   },
   "source": [
    "### Tolerancia a fallos\n",
    "\n",
    "En el entrenamiento síncrono, el cluster fallaría si uno de los trabajadores falla y no existe ningún mecanismo de recuperación de fallos.\n",
    "\n",
    "Usar Keras con `tf.distribute.Strategy` ofrece la ventaja de la tolerancia a fallos cuando los trabajadores mueren o son inestables por algún otro motivo. Puede hacerlo conservando el estado de entrenamiento en el sistema de archivos distribuido de su elección, de forma que al reiniciar la instancia que falló o se adelantó previamente, se recupere el estado de entrenamiento.\n",
    "\n",
    "Cuando un trabajador deja de estar disponible, otros trabajadores fallarán (posiblemente tras un tiempo de espera). En tales casos, es necesario reiniciar el trabajador no disponible, así como otros trabajadores que hayan fallado.\n",
    "\n",
    "Nota: Anteriormente, la retrollamada `ModelCheckpoint` servía de mecanismo para restaurar el estado de entrenamiento tras un reinicio por fallo de un trabajo para el entrenamiento multitrabajador. El equipo de TensorFlow está añadiendo una nueva retrollamada [`BackupAndRestore`](#scrollTo=kmH8uCUhfn4w), que añade también el soporte al entrenamiento de un solo trabajador para una experiencia consistente, y ha eliminado la funcionalidad de tolerancia a fallos de la retrollamada `ModelCheckpoint` existente. A partir de ahora, las aplicaciones que dependan de este comportamiento deberán migrar a la nueva retrollamada `BackupAndRestore`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvHPjGlyyFt6"
   },
   "source": [
    "#### La retrollamada `ModelCheckpoint`\n",
    "\n",
    "La retrollamada `ModelCheckpoint` ya no ofrece la funcionalidad de tolerancia a fallos, así que use en su lugar la retrollamada [`BackupAndRestore`](#scrollTo=kmH8uCUhfn4w).\n",
    "\n",
    "La retrollamada `ModelCheckpoint` aún puede usarse para guardar puntos de verificación. Sin embargo, si el entrenamiento se interrumpió o finalizó con éxito, para continuar el entrenamiento a partir del punto de verificación, el usuario es responsable de cargar el modelo manualmente.\n",
    "\n",
    "De forma opcional, los usuarios pueden elegir guardar y restaurar el modelo/las ponderaciones fuera de la retrollamada `ModelCheckpoint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EUNV5Utc1d0s"
   },
   "source": [
    "### Guardar y cargar modelos\n",
    "\n",
    "Para guardar su modelo usando `model.save` o `tf.saved_model.save`, es necesario que la dirección de guardado sea diferente para cada trabajador.\n",
    "\n",
    "- Para los trabajadores que no sean jefes, tendrá que guardar el modelo en un directorio temporal.\n",
    "- Para el jefe, tendrá que guardar en el directorio de modelos proporcionado.\n",
    "\n",
    "Los directorios temporales del trabajador deben ser únicos para evitar errores resultantes de que varios trabajadores intenten escribir en la misma ubicación.\n",
    "\n",
    "El modelo guardado en todos los directorios es idéntico, y por lo general sólo hay que consultar el modelo guardado por el jefe para restaurarlo o usarlo.\n",
    "\n",
    "Debe tener alguna lógica de limpieza que borre los directorios temporales creados por los trabajadores una vez que su entrenamiento se haya llenado.\n",
    "\n",
    "La razón para guardar en el jefe y en los trabajadores al mismo tiempo es que podría estar agregando variables durante el punto de verificación, lo que requiere que tanto el jefe como los trabajadores participen en el protocolo de comunicación allreduce. Por otro lado, dejar que el jefe y los trabajadores guarden en el mismo directorio del modelo resultará en errores debido a la contención.\n",
    "\n",
    "Usando la `MultiWorkerMirroredStrategy`, el programa se ejecuta en cada trabajador, y para saber si el trabajador actual es el jefe, aprovecha el objeto resolutor del cluster que tiene atributos `task_type` y `task_id`:\n",
    "\n",
    "- `task_type` le indica cuál es el trabajo actual (por ejemplo, `'worker'`).\n",
    "- `task_id` le indica el identificador del trabajador.\n",
    "- El trabajador con `task_id == 0` es designado como trabajador jefe.\n",
    "\n",
    "En el fragmento de código siguiente, la función `write_filepath` indica la ruta del archivo a escribir, que depende del `task_id` del trabajador:\n",
    "\n",
    "- Para el trabajador jefe (con `task_id == 0`), escribe en la ruta del archivo original.\n",
    "- Para otros trabajadores, crea un directorio temporal (`temp_dir`) con el `task_id` en la ruta del directorio para escribir en él:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XQfGkmg-pfCY"
   },
   "outputs": [],
   "source": [
    "model_path = '/tmp/keras-model'\n",
    "\n",
    "def _is_chief(task_type, task_id):\n",
    "  # Note: there are two possible `TF_CONFIG` configurations.\n",
    "  #   1) In addition to `worker` tasks, a `chief` task type is use;\n",
    "  #      in this case, this function should be modified to\n",
    "  #      `return task_type == 'chief'`.\n",
    "  #   2) Only `worker` task type is used; in this case, worker 0 is\n",
    "  #      regarded as the chief. The implementation demonstrated here\n",
    "  #      is for this case.\n",
    "  # For the purpose of this Colab section, the `task_type` is `None` case\n",
    "  # is added because it is effectively run with only a single worker.\n",
    "  return (task_type == 'worker' and task_id == 0) or task_type is None\n",
    "\n",
    "def _get_temp_dir(dirpath, task_id):\n",
    "  base_dirpath = 'workertemp_' + str(task_id)\n",
    "  temp_dir = os.path.join(dirpath, base_dirpath)\n",
    "  tf.io.gfile.makedirs(temp_dir)\n",
    "  return temp_dir\n",
    "\n",
    "def write_filepath(filepath, task_type, task_id):\n",
    "  dirpath = os.path.dirname(filepath)\n",
    "  base = os.path.basename(filepath)\n",
    "  if not _is_chief(task_type, task_id):\n",
    "    dirpath = _get_temp_dir(dirpath, task_id)\n",
    "  return os.path.join(dirpath, base)\n",
    "\n",
    "task_type, task_id = (strategy.cluster_resolver.task_type,\n",
    "                      strategy.cluster_resolver.task_id)\n",
    "write_model_path = write_filepath(model_path, task_type, task_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hs0_agYR_qKm"
   },
   "source": [
    "Con eso, ya está listo para guardar:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XnToxeIcg_6O"
   },
   "source": [
    "Obsoleto: Para los objetos Keras, se recomienda usar el nuevo formato de alto nivel `.keras` y `tf.keras.Model.export`, como se demuestra en la guía [aquí](https://www.tensorflow.org/guide/keras/save_and_serialize). El formato SavedModel de bajo nivel sigue siendo compatible con el código existente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-yA3BYG_vTs"
   },
   "outputs": [],
   "source": [
    "multi_worker_model.save(write_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8LXUVVl9_v5x"
   },
   "source": [
    "Como ya se ha descrito, más adelante el modelo sólo debe cargarse desde la ruta del archivo en el que guardó el trabajador jefe. Por lo tanto, elimine los temporales que hayan guardado los trabajadores no jefes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aJTyu-97ABpY"
   },
   "outputs": [],
   "source": [
    "if not _is_chief(task_type, task_id):\n",
    "  tf.io.gfile.rmtree(os.path.dirname(write_model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nr-2PKlHAPBT"
   },
   "source": [
    "Ahora, cuando sea el momento de cargar, use la práctica API `tf.keras.models.load_model`, y continúe con el trabajo posterior.\n",
    "\n",
    "Aquí, supongamos que sólo se usa un único trabajador para cargar y continuar el entrenamiento, en cuyo caso no se llama a `tf.keras.models.load_model` dentro de otro `strategy.scope()` (tenga en cuenta que `strategy = tf.distribute.MultiWorkerMirroredStrategy()`, como se definió anteriormente):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUZna-JKAOrX"
   },
   "outputs": [],
   "source": [
    "loaded_model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Now that the model is restored, and can continue with the training.\n",
    "loaded_model.fit(single_worker_dataset, epochs=2, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJ1fmxmTpocS"
   },
   "source": [
    "### Guardado y restauración del punto de verificación\n",
    "\n",
    "Por otro lado, el punto de verificación le permite guardar las ponderaciones de su modelo y restaurarlas sin tener que guardar todo el modelo.\n",
    "\n",
    "Aquí, creará un `tf.train.Checkpoint` que realice el seguimiento del modelo, que será administrado por el `tf.train.CheckpointManager`, de forma que sólo se conserve el último punto de verificación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_1-RYaB5xnNH"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir = '/tmp/ckpt'\n",
    "\n",
    "checkpoint = tf.train.Checkpoint(model=multi_worker_model)\n",
    "write_checkpoint_dir = write_filepath(checkpoint_dir, task_type, task_id)\n",
    "checkpoint_manager = tf.train.CheckpointManager(\n",
    "    checkpoint, directory=write_checkpoint_dir, max_to_keep=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oBpPCRsW1MF"
   },
   "source": [
    "Una vez configurado el `CheckpointManager`, ya está listo para guardar y eliminar los puntos de verificación que hayan guardado los trabajadores no jefes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l1ZXG_GbWzLp"
   },
   "outputs": [],
   "source": [
    "checkpoint_manager.save()\n",
    "if not _is_chief(task_type, task_id):\n",
    "  tf.io.gfile.rmtree(write_checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RO7cbN40XD5v"
   },
   "source": [
    "Ahora, cuando necesite restaurar el modelo, puede encontrar el último punto de verificación guardado usando la práctica función `tf.train.latest_checkpoint`. Tras restaurar el punto de verificación, puede continuar con el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJW7vtknXFEH"
   },
   "outputs": [],
   "source": [
    "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "checkpoint.restore(latest_checkpoint)\n",
    "multi_worker_model.fit(multi_worker_dataset, epochs=2, steps_per_epoch=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmH8uCUhfn4w"
   },
   "source": [
    "#### La retrollamada `BackupAndRestore`\n",
    "\n",
    "La retrollamada `tf.keras.callbacks.BackupAndRestore` ofrece la funcionalidad de tolerancia a fallos haciendo una copia de seguridad del modelo y del estado actual del entrenamiento en un archivo temporal de puntos de verificación bajo el argumento `backup_dir` a `BackupAndRestore`.\n",
    "\n",
    "Nota: En Tensorflow 2.9, se realiza una copia de seguridad del modelo actual y del estado de entrenamiento en los límites de las épocas. En la versión `tf-nightly` y a partir de TensorFlow 2.10, la retrollamada `BackupAndRestore` puede realizar una copia de seguridad del modelo y del estado de entrenamiento en los límites de época o de paso. `BackupAndRestore` acepta un argumento opcional `save_freq`. `save_freq` acepta ya sea `'epoch'` o un valor `int`. Si `save_freq` se configura como `'epoch'` se realiza una copia de seguridad del modelo después de cada época. Si `save_freq` se configura como valor entero superior a `0`, se realiza una copia de seguridad del modelo después de cada `save_freq` número de lotes.\n",
    "\n",
    "Una vez que los trabajos se interrumpen y se reinician, la retrollamada `BackupAndRestore` restaura el último punto de verificación, y puede continuar el entrenamiento desde el principio de la época y el paso en los que se guardó por última vez el estado de entrenamiento.\n",
    "\n",
    "Para usarlo, facilite una instancia de `tf.keras.callbacks.BackupAndRestore` en la llamada `Model.fit`.\n",
    "\n",
    "Con `MultiWorkerMirroredStrategy`, si un trabajador es interrumpido, todo el cluster hará una pausa hasta que el trabajador interrumpido vuelva a arrancar. Los demás trabajadores también se reiniciarán y el trabajador interrumpido volverá a unirse al clúster. Entonces, cada trabajador leerá el archivo de punto de verificación que se guardó previamente y retomará su estado anterior, permitiendo así que el clúster vuelva a entrar en sincronía. A continuación, continuará el entrenamiento. El estado del iterador del conjunto de datos distribuido se reiniciará y no se restaurará.\n",
    "\n",
    "La retrollamada `BackupAndRestore` usa el `CheckpointManager` para guardar y restaurar el estado de entrenamiento, lo que genera un archivo llamado checkpoint que hace un seguimiento de los puntos de verificación existentes junto con el más reciente. Por este motivo, `backup_dir` no debe usarse para almacenar otros puntos de verificación a fin de evitar la colisión de nombres.\n",
    "\n",
    "Actualmente, la retrollamada `BackupAndRestore` admite el entrenamiento de un solo trabajador sin estrategia (`MirroredStrategy`) y el entrenamiento multitrabajador con `MultiWorkerMirroredStrategy`.\n",
    "\n",
    "He aquí dos ejemplos tanto para el entrenamiento de multitrabajadores como para el de un solo trabajador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CYdzZi4Qs1jz"
   },
   "outputs": [],
   "source": [
    "# Multi-worker training with `MultiWorkerMirroredStrategy`\n",
    "# and the `BackupAndRestore` callback. The training state \n",
    "# is backed up at epoch boundaries by default.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8e86TAp0Rsl"
   },
   "source": [
    "Si el argumento `save_freq` de la retrollamada `BackupAndRestore` se configura como `'epoch'`, se realiza una copia de seguridad del modelo después de cada época."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZjQGPsF0aEI"
   },
   "outputs": [],
   "source": [
    "# The training state is backed up at epoch boundaries because `save_freq` is\n",
    "# set to `epoch`.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup')]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-r44kCM0jc6"
   },
   "source": [
    "Nota: El siguiente bloque de código usa funciones que sólo disponibles en `tf-nightly` hasta que se publique Tensorflow 2.10.\n",
    "\n",
    "Si el argumento `save_freq` de la retrollamada `BackupAndRestore` se configura en un valor entero superior a `0`, se hace una copia de seguridad del modelo después de cada `save_freq` número de lotes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bSJUyLSF0moC"
   },
   "outputs": [],
   "source": [
    "# The training state is backed up at every 30 steps because `save_freq` is set\n",
    "# to an integer value of `30`.\n",
    "\n",
    "callbacks = [tf.keras.callbacks.BackupAndRestore(backup_dir='/tmp/backup', save_freq=30)]\n",
    "with strategy.scope():\n",
    "  multi_worker_model = mnist_setup.build_and_compile_cnn_model()\n",
    "multi_worker_model.fit(multi_worker_dataset,\n",
    "                       epochs=3,\n",
    "                       steps_per_epoch=70,\n",
    "                       callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIV5_3ebzXmB"
   },
   "source": [
    "Si examina el directorio de `backup_dir` que especificó en `BackupAndRestore`, puede que note algunos archivos de puntos de verificación generados temporalmente. Esos archivos son necesarios para recuperar las instancias perdidas anteriormente, y serán eliminados por la librería al final de `Model.fit` al salir con éxito de su entrenamiento.\n",
    "\n",
    "Nota: Actualmente la retrollamada `BackupAndRestore` sólo soporta eager mode. En modo grafo, considere usar `Model.save`/`tf.saved_model.save` y `tf.keras.models.load_model` para guardar y recuperar modelos, respectivamente, como se describe en la sección *Guardar y cargar modelos* anterior, y proporcionando `initial_epoch` en `Model.fit` durante el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ega2hdOQEmy_"
   },
   "source": [
    "## Recursos adicionales\n",
    "\n",
    "1. La guía [Entrenamiento distribuido en TensorFlow](../../guide/distributed_training.ipynb) proporciona una visión general de las estrategias de distribución disponibles.\n",
    "2. El tutorial [Bucle de entrenamiento personalizado con Keras y MultiWorkerMirroredStrategy](multi_worker_with_ctl.ipynb) muestra cómo usar la `MultiWorkerMirroredStrategy` con Keras y un bucle de entrenamiento personalizado.\n",
    "3. Consulte los [modelos oficiales](https://github.com/tensorflow/models/tree/master/official), muchos de los cuales pueden configurarse para ejecutar múltiples estrategias de distribución.\n",
    "4. La guía [Un mejor rendimiento con tf.function](../../guide/function.ipynb) ofrece información sobre otras estrategias y herramientas, como el [Perfilador TensorFlow](../../guide/profiler.md) que puede usar para optimizar el rendimiento de sus modelos TensorFlow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TBFXQGKYUc4X"
   },
   "source": [
    "##### Copyright 2022 The TensorFlow Authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "1z4xy2gTUc4a"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KwQtSOz0VrVX"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/video/video_classification\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/video/video_classification.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L2MHy42s5wl6"
   },
   "source": [
    "# Clasificación de videos con una red neuronal convolucional 3D\n",
    "\n",
    "Este tutorial demuestra el entrenamiento de una red neuronal convolucional 3D (CNN) para la clasificación de vídeos utilizando el conjunto de datos de reconocimiento de acciones [UCF101](https://www.crcv.ucf.edu/data/UCF101.php). Una CNN 3D utiliza un filtro tridimensional para realizar las convoluciones. El núcleo puede deslizarse en tres direcciones, mientras que en una CNN 2D puede deslizarse en dos dimensiones. El modelo se basa en el trabajo publicado en [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248v3) por D. Tran et al. (2017).  En este tutorial, usted:\n",
    "\n",
    "- Construirá una canalización de entrada\n",
    "- Construirá un modelo de red neuronal convolucional 3D con conexiones residuales usando la API funcional de Keras\n",
    "- Entrenará el modelo\n",
    "- Evaluará y probará el modelo\n",
    "\n",
    "Este video tutorial de clasificación es la segunda parte de una serie de video tutoriales de TensorFlow. Aquí están los otros tres tutoriales:\n",
    "\n",
    "- [Cargar datos de vídeo](https://www.tensorflow.org/tutorials/load_data/video): Este tutorial explica gran parte del código usado en este documento.\n",
    "- [MoViNet para el reconocimiento de acciones en streaming](https://www.tensorflow.org/hub/tutorials/movinet): Familiarícese con los modelos MoViNet disponibles en TF Hub.\n",
    "- [Aprendizaje por transferencia para la clasificación de vídeo con MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet): Este tutorial explica cómo usar un modelo de clasificación de vídeo preentrenado y entrenado en un conjunto de datos diferente con el conjunto de datos UCF-101."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Ih_df2q0kw4"
   },
   "source": [
    "## Preparación\n",
    "\n",
    "Comience instalando e importando algunas librerías necesarias, incluyendo: [remotezip](https://github.com/gtsystem/python-remotezip) para inspeccionar el contenido de un archivo ZIP, [tqdm](https://github.com/tqdm/tqdm) para usar una barra de progreso, [OpenCV](https://opencv.org/) para procesar archivos de vídeo, [einops](https://github.com/arogozhnikov/einops/tree/master/docs) para realizar operaciones tensoriales más complejas, y [`tensorflow_docs`](https://github.com/tensorflow/docs/tree/master/tools/tensorflow_docs) para incorporar datos en un bloc de notas de Jupyter.\n",
    "\n",
    "**Nota**: Use TensorFlow 2.10 para ejecutar este tutorial. Es posible que las versiones superiores a TensorFlow 2.10 no se ejecuten correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KEbL4Mwi01PV"
   },
   "outputs": [],
   "source": [
    "!pip install remotezip tqdm opencv-python einops \n",
    "# Install TensorFlow 2.10\n",
    "!pip install tensorflow==2.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gg0otuqb0hIf"
   },
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import random\n",
    "import pathlib\n",
    "import itertools\n",
    "import collections\n",
    "\n",
    "import cv2\n",
    "import einops\n",
    "import numpy as np\n",
    "import remotezip as rz\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ctk9A57-6ABq"
   },
   "source": [
    "## Carga y procesamiento de datos de video\n",
    "\n",
    "La celda oculta inferior define funciones ayudantes para descargar una porción de datos del conjunto de datos UCF-101 y cargarla en un `tf.data.Dataset`. Puede aprender más sobre los pasos específicos del preprocesamiento en el tutorial [Carga de datos de video](../load_data/video.ipynb), que le guiará a través de este código con más detalle.\n",
    "\n",
    "La clase `FrameGenerator` al final del bloque oculto es la utilidad más importante en este caso. Crea un objeto iterable que puede alimentar los datos en la canalización de datos de TensorFlow. Específicamente, esta clase contiene un generador Python que carga los cuadros de video junto con su etiqueta codificada. La función del generador (`__call__`) produce el arreglo del marco emitido por `frames_from_video_file` y un vector codificado en un solo paso (one-hot) de la etiqueta asociada con el conjunto de cuadros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nB2aOTU35r9_"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "def list_files_per_class(zip_url):\n",
    "  \"\"\"\n",
    "    List the files in each class of the dataset given the zip URL.\n",
    "\n",
    "    Args:\n",
    "      zip_url: URL from which the files can be unzipped. \n",
    "\n",
    "    Return:\n",
    "      files: List of files in each of the classes.\n",
    "  \"\"\"\n",
    "  files = []\n",
    "  with rz.RemoteZip(URL) as zip:\n",
    "    for zip_info in zip.infolist():\n",
    "      files.append(zip_info.filename)\n",
    "  return files\n",
    "\n",
    "def get_class(fname):\n",
    "  \"\"\"\n",
    "    Retrieve the name of the class given a filename.\n",
    "\n",
    "    Args:\n",
    "      fname: Name of the file in the UCF101 dataset.\n",
    "\n",
    "    Return:\n",
    "      Class that the file belongs to.\n",
    "  \"\"\"\n",
    "  return fname.split('_')[-3]\n",
    "\n",
    "def get_files_per_class(files):\n",
    "  \"\"\"\n",
    "    Retrieve the files that belong to each class. \n",
    "\n",
    "    Args:\n",
    "      files: List of files in the dataset.\n",
    "\n",
    "    Return:\n",
    "      Dictionary of class names (key) and files (values).\n",
    "  \"\"\"\n",
    "  files_for_class = collections.defaultdict(list)\n",
    "  for fname in files:\n",
    "    class_name = get_class(fname)\n",
    "    files_for_class[class_name].append(fname)\n",
    "  return files_for_class\n",
    "\n",
    "def download_from_zip(zip_url, to_dir, file_names):\n",
    "  \"\"\"\n",
    "    Download the contents of the zip file from the zip URL.\n",
    "\n",
    "    Args:\n",
    "      zip_url: Zip URL containing data.\n",
    "      to_dir: Directory to download data to.\n",
    "      file_names: Names of files to download.\n",
    "  \"\"\"\n",
    "  with rz.RemoteZip(zip_url) as zip:\n",
    "    for fn in tqdm.tqdm(file_names):\n",
    "      class_name = get_class(fn)\n",
    "      zip.extract(fn, str(to_dir / class_name))\n",
    "      unzipped_file = to_dir / class_name / fn\n",
    "\n",
    "      fn = pathlib.Path(fn).parts[-1]\n",
    "      output_file = to_dir / class_name / fn\n",
    "      unzipped_file.rename(output_file,)\n",
    "\n",
    "def split_class_lists(files_for_class, count):\n",
    "  \"\"\"\n",
    "    Returns the list of files belonging to a subset of data as well as the remainder of\n",
    "    files that need to be downloaded.\n",
    "    \n",
    "    Args:\n",
    "      files_for_class: Files belonging to a particular class of data.\n",
    "      count: Number of files to download.\n",
    "\n",
    "    Return:\n",
    "      split_files: Files belonging to the subset of data.\n",
    "      remainder: Dictionary of the remainder of files that need to be downloaded.\n",
    "  \"\"\"\n",
    "  split_files = []\n",
    "  remainder = {}\n",
    "  for cls in files_for_class:\n",
    "    split_files.extend(files_for_class[cls][:count])\n",
    "    remainder[cls] = files_for_class[cls][count:]\n",
    "  return split_files, remainder\n",
    "\n",
    "def download_ufc_101_subset(zip_url, num_classes, splits, download_dir):\n",
    "  \"\"\"\n",
    "    Download a subset of the UFC101 dataset and split them into various parts, such as\n",
    "    training, validation, and test. \n",
    "\n",
    "    Args:\n",
    "      zip_url: Zip URL containing data.\n",
    "      num_classes: Number of labels.\n",
    "      splits: Dictionary specifying the training, validation, test, etc. (key) division of data \n",
    "              (value is number of files per split).\n",
    "      download_dir: Directory to download data to.\n",
    "\n",
    "    Return:\n",
    "      dir: Posix path of the resulting directories containing the splits of data.\n",
    "  \"\"\"\n",
    "  files = list_files_per_class(zip_url)\n",
    "  for f in files:\n",
    "    tokens = f.split('/')\n",
    "    if len(tokens) <= 2:\n",
    "      files.remove(f) # Remove that item from the list if it does not have a filename\n",
    "  \n",
    "  files_for_class = get_files_per_class(files)\n",
    "\n",
    "  classes = list(files_for_class.keys())[:num_classes]\n",
    "\n",
    "  for cls in classes:\n",
    "    new_files_for_class = files_for_class[cls]\n",
    "    random.shuffle(new_files_for_class)\n",
    "    files_for_class[cls] = new_files_for_class\n",
    "    \n",
    "  # Only use the number of classes you want in the dictionary\n",
    "  files_for_class = {x: files_for_class[x] for x in list(files_for_class)[:num_classes]}\n",
    "\n",
    "  dirs = {}\n",
    "  for split_name, split_count in splits.items():\n",
    "    print(split_name, \":\")\n",
    "    split_dir = download_dir / split_name\n",
    "    split_files, files_for_class = split_class_lists(files_for_class, split_count)\n",
    "    download_from_zip(zip_url, split_dir, split_files)\n",
    "    dirs[split_name] = split_dir\n",
    "\n",
    "  return dirs\n",
    "\n",
    "def format_frames(frame, output_size):\n",
    "  \"\"\"\n",
    "    Pad and resize an image from a video.\n",
    "    \n",
    "    Args:\n",
    "      frame: Image that needs to resized and padded. \n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      Formatted frame with padding of specified output size.\n",
    "  \"\"\"\n",
    "  frame = tf.image.convert_image_dtype(frame, tf.float32)\n",
    "  frame = tf.image.resize_with_pad(frame, *output_size)\n",
    "  return frame\n",
    "\n",
    "def frames_from_video_file(video_path, n_frames, output_size = (224,224), frame_step = 15):\n",
    "  \"\"\"\n",
    "    Creates frames from each video file present for each category.\n",
    "\n",
    "    Args:\n",
    "      video_path: File path to the video.\n",
    "      n_frames: Number of frames to be created per video file.\n",
    "      output_size: Pixel size of the output frame image.\n",
    "\n",
    "    Return:\n",
    "      An NumPy array of frames in the shape of (n_frames, height, width, channels).\n",
    "  \"\"\"\n",
    "  # Read each video frame by frame\n",
    "  result = []\n",
    "  src = cv2.VideoCapture(str(video_path))  \n",
    "\n",
    "  video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "\n",
    "  need_length = 1 + (n_frames - 1) * frame_step\n",
    "\n",
    "  if need_length > video_length:\n",
    "    start = 0\n",
    "  else:\n",
    "    max_start = video_length - need_length\n",
    "    start = random.randint(0, max_start + 1)\n",
    "\n",
    "  src.set(cv2.CAP_PROP_POS_FRAMES, start)\n",
    "  # ret is a boolean indicating whether read was successful, frame is the image itself\n",
    "  ret, frame = src.read()\n",
    "  result.append(format_frames(frame, output_size))\n",
    "\n",
    "  for _ in range(n_frames - 1):\n",
    "    for _ in range(frame_step):\n",
    "      ret, frame = src.read()\n",
    "    if ret:\n",
    "      frame = format_frames(frame, output_size)\n",
    "      result.append(frame)\n",
    "    else:\n",
    "      result.append(np.zeros_like(result[0]))\n",
    "  src.release()\n",
    "  result = np.array(result)[..., [2, 1, 0]]\n",
    "\n",
    "  return result\n",
    "\n",
    "class FrameGenerator:\n",
    "  def __init__(self, path, n_frames, training = False):\n",
    "    \"\"\" Returns a set of frames with their associated label. \n",
    "\n",
    "      Args:\n",
    "        path: Video file paths.\n",
    "        n_frames: Number of frames. \n",
    "        training: Boolean to determine if training dataset is being created.\n",
    "    \"\"\"\n",
    "    self.path = path\n",
    "    self.n_frames = n_frames\n",
    "    self.training = training\n",
    "    self.class_names = sorted(set(p.name for p in self.path.iterdir() if p.is_dir()))\n",
    "    self.class_ids_for_name = dict((name, idx) for idx, name in enumerate(self.class_names))\n",
    "\n",
    "  def get_files_and_class_names(self):\n",
    "    video_paths = list(self.path.glob('*/*.avi'))\n",
    "    classes = [p.parent.name for p in video_paths] \n",
    "    return video_paths, classes\n",
    "\n",
    "  def __call__(self):\n",
    "    video_paths, classes = self.get_files_and_class_names()\n",
    "\n",
    "    pairs = list(zip(video_paths, classes))\n",
    "\n",
    "    if self.training:\n",
    "      random.shuffle(pairs)\n",
    "\n",
    "    for path, name in pairs:\n",
    "      video_frames = frames_from_video_file(path, self.n_frames) \n",
    "      label = self.class_ids_for_name[name] # Encode labels\n",
    "      yield video_frames, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OYY7PkdJFM4Z"
   },
   "outputs": [],
   "source": [
    "URL = 'https://storage.googleapis.com/thumos14_files/UCF101_videos.zip'\n",
    "download_dir = pathlib.Path('./UCF101_subset/')\n",
    "subset_paths = download_ufc_101_subset(URL, \n",
    "                        num_classes = 10, \n",
    "                        splits = {\"train\": 30, \"val\": 10, \"test\": 10},\n",
    "                        download_dir = download_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C0O3ttIzpFZJ"
   },
   "source": [
    "Cree los conjuntos de entrenamiento, validación y prueba (`train_ds`, `val_ds` y `test_ds`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "lq86IyGDJjTX"
   },
   "outputs": [],
   "source": [
    "n_frames = 10\n",
    "batch_size = 8\n",
    "\n",
    "output_signature = (tf.TensorSpec(shape = (None, None, None, 3), dtype = tf.float32),\n",
    "                    tf.TensorSpec(shape = (), dtype = tf.int16))\n",
    "\n",
    "train_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['train'], n_frames, training=True),\n",
    "                                          output_signature = output_signature)\n",
    "\n",
    "\n",
    "# Batch the data\n",
    "train_ds = train_ds.batch(batch_size)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['val'], n_frames),\n",
    "                                        output_signature = output_signature)\n",
    "val_ds = val_ds.batch(batch_size)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_generator(FrameGenerator(subset_paths['test'], n_frames),\n",
    "                                         output_signature = output_signature)\n",
    "\n",
    "test_ds = test_ds.batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nzogoGA4pQW0"
   },
   "source": [
    "## Crear el modelo\n",
    "\n",
    "El siguiente modelo de red neuronal convolucional 3D se basa en el artículo [A Closer Look at Spatiotemporal Convolutions for Action Recognition](https://arxiv.org/abs/1711.11248v3) de D. Tran et al. (2017). El artículo compara varias versiones de ResNets 3D. En lugar de operar sobre una sola imagen con dimensiones `(height, width)`, como las ResNets estándares, estas operan sobre el volumen de vídeo `(time, height, width)`. El enfoque más obvio a este problema sería reemplazar cada (`layers.Conv2D`) de convolución 2D por una convolución 3D (`layers.Conv3D`).\n",
    "\n",
    "Este tutorial usa una convolución (2 + 1)D con [conexiones residuales](https://arxiv.org/abs/1512.03385). La convolución (2 + 1)D permite descomponer las dimensiones espacial y temporal, creando así dos pasos separados. Una ventaja de este enfoque es que la factorización de las convoluciones en dimensiones espaciales y temporales ahorra parámetros.\n",
    "\n",
    "Para cada ubicación de salida, una convolución 3D combina todos los vectores de un parche 3D del volumen para crear un vector en el volumen de salida.\n",
    "\n",
    "![3D convolutions](https://www.tensorflow.org/images/tutorials/video/3DCNN.png)\n",
    "\n",
    "Esta operación toma entradas `time * height * width * channels` y produce salidas `channels` (suponiendo que el número de canales de entrada y de salida sea el mismo. Así, una capa de convolución 3D con un tamaño de kernel de `(3 x 3 x 3)` necesitaría una matriz de ponderación con entradas de `27 * canales ** 2`. El documento de referencia encontró que un enfoque más eficaz y eficiente era factorizar la convolución. En lugar de una única convolución 3D para procesar las dimensiones temporal y espacial, propusieron una convolución \"(2+1)D\" que procesa las dimensiones espacial y temporal por separado. La figura siguiente muestra las convoluciones espacial y temporal factorizadas de una convolución (2 + 1)D.\n",
    "\n",
    "![(2+1)D convolutions](https://www.tensorflow.org/images/tutorials/video/2plus1CNN.png)\n",
    "\n",
    "La principal ventaja de este enfoque es que reduce el número de parámetros. En la convolución (2 + 1)D, la convolución espacial toma datos de la forma `(1, width, height)`, mientras que la convolución temporal toma datos de la forma `(time, 1, 1)`. Por ejemplo, una convolución (2 + 1)D con tamaño de kernel `(3 x 3 x 3)` necesitaría matrices de ponderación de tamaño `(9 * channels**2) + (3 * channels**2)`, menos de la mitad que la convolución 3D completa. Este tutorial implementa la ResNet18 (2 + 1)D, en la que cada convolución de la resnet se sustituye por una convolución (2+1)D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GZcB_7dg-EZJ"
   },
   "outputs": [],
   "source": [
    "# Define the dimensions of one frame in the set of frames created\n",
    "HEIGHT = 224\n",
    "WIDTH = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yD_sDIBlNu7K"
   },
   "outputs": [],
   "source": [
    "class Conv2Plus1D(keras.layers.Layer):\n",
    "  def __init__(self, filters, kernel_size, padding):\n",
    "    \"\"\"\n",
    "      A sequence of convolutional layers that first apply the convolution operation over the\n",
    "      spatial dimensions, and then the temporal dimension. \n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([  \n",
    "        # Spatial decomposition\n",
    "        layers.Conv3D(filters=filters,\n",
    "                      kernel_size=(1, kernel_size[1], kernel_size[2]),\n",
    "                      padding=padding),\n",
    "        # Temporal decomposition\n",
    "        layers.Conv3D(filters=filters, \n",
    "                      kernel_size=(kernel_size[0], 1, 1),\n",
    "                      padding=padding)\n",
    "        ])\n",
    "  \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-fCAddqEORZ"
   },
   "source": [
    "Un modelo ResNet se elabora a partir de una secuencia de bloques residuales. Un bloque residual tiene dos ramas. La rama principal realiza el cálculo, pero dificulta el paso de los gradientes. La derivación residual pasa por alto el cálculo principal y, en la mayoría de los casos, se limita a añadir la entrada a la salida de la rama principal. Los gradientes fluyen fácilmente a través de esta rama. Por lo tanto, habrá una ruta fácil desde la función de pérdida hasta cualquiera de las ramas principales del bloque residual. Esto evita el problema del gradiente evanescente.\n",
    "\n",
    "Cree la rama principal del bloque residual con la siguiente clase. A diferencia de la estructura estándar de ResNet, ésta utiliza la capa personalizada `Conv2Plus1D` en lugar de `layers.Conv2D`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tjxAKHwn6mTJ"
   },
   "outputs": [],
   "source": [
    "class ResidualMain(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Residual block of the model with convolution, layer normalization, and the\n",
    "    activation function, ReLU.\n",
    "  \"\"\"\n",
    "  def __init__(self, filters, kernel_size):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        Conv2Plus1D(filters=filters,\n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization(),\n",
    "        layers.ReLU(),\n",
    "        Conv2Plus1D(filters=filters, \n",
    "                    kernel_size=kernel_size,\n",
    "                    padding='same'),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "    \n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CevmZ9qsdpWC"
   },
   "source": [
    "Para añadir la rama residual a la rama principal es necesario que tenga el mismo tamaño. La capa `Project` inferior se ocupa de los casos en los que se modifica el número de canales en la derivación. En concreto, se añade una secuencia de capa densamente conectada seguida de normalización. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "znrk5BrL6kuq"
   },
   "outputs": [],
   "source": [
    "class Project(keras.layers.Layer):\n",
    "  \"\"\"\n",
    "    Project certain dimensions of the tensor as the data is passed through different \n",
    "    sized filters and downsampled. \n",
    "  \"\"\"\n",
    "  def __init__(self, units):\n",
    "    super().__init__()\n",
    "    self.seq = keras.Sequential([\n",
    "        layers.Dense(units),\n",
    "        layers.LayerNormalization()\n",
    "    ])\n",
    "\n",
    "  def call(self, x):\n",
    "    return self.seq(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S8zycXGvfnak"
   },
   "source": [
    "Use `add_residual_block` para crear una conexión de salto entre las capas del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urjVgqvw-TlB"
   },
   "outputs": [],
   "source": [
    "def add_residual_block(input, filters, kernel_size):\n",
    "  \"\"\"\n",
    "    Add residual blocks to the model. If the last dimensions of the input data\n",
    "    and filter size does not match, project it such that last dimension matches.\n",
    "  \"\"\"\n",
    "  out = ResidualMain(filters, \n",
    "                     kernel_size)(input)\n",
    "  \n",
    "  res = input\n",
    "  # Using the Keras functional APIs, project the last dimension of the tensor to\n",
    "  # match the new filter size\n",
    "  if out.shape[-1] != input.shape[-1]:\n",
    "    res = Project(out.shape[-1])(res)\n",
    "\n",
    "  return layers.add([res, out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bozog_0hFKrD"
   },
   "source": [
    "Es necesario cambiar el tamaño del vídeo para realizar un downsampling de los datos. En concreto, el downsampling de los fotogramas de vídeo permite que el modelo examine partes concretas de los fotogramas para detectar patrones que puedan ser específicos de una determinada acción. Mediante el downsampling, se puede descartar la información no esencial. Además, redimensionar el vídeo permitirá reducir la dimensionalidad y, por tanto, acelerar el procesamiento a través del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQOWuc2I-QqK"
   },
   "outputs": [],
   "source": [
    "class ResizeVideo(keras.layers.Layer):\n",
    "  def __init__(self, height, width):\n",
    "    super().__init__()\n",
    "    self.height = height\n",
    "    self.width = width\n",
    "    self.resizing_layer = layers.Resizing(self.height, self.width)\n",
    "\n",
    "  def call(self, video):\n",
    "    \"\"\"\n",
    "      Use the einops library to resize the tensor.  \n",
    "      \n",
    "      Args:\n",
    "        video: Tensor representation of the video, in the form of a set of frames.\n",
    "      \n",
    "      Return:\n",
    "        A downsampled size of the video according to the new height and width it should be resized to.\n",
    "    \"\"\"\n",
    "    # b stands for batch size, t stands for time, h stands for height, \n",
    "    # w stands for width, and c stands for the number of channels.\n",
    "    old_shape = einops.parse_shape(video, 'b t h w c')\n",
    "    images = einops.rearrange(video, 'b t h w c -> (b t) h w c')\n",
    "    images = self.resizing_layer(images)\n",
    "    videos = einops.rearrange(\n",
    "        images, '(b t) h w c -> b t h w c',\n",
    "        t = old_shape['t'])\n",
    "    return videos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z9IqzCq--Uu9"
   },
   "source": [
    "Usa la API funcional [Keras](https://www.tensorflow.org/guide/keras/functional) para construir la red residual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_bROfh_K-Wxs"
   },
   "outputs": [],
   "source": [
    "input_shape = (None, 10, HEIGHT, WIDTH, 3)\n",
    "input = layers.Input(shape=(input_shape[1:]))\n",
    "x = input\n",
    "\n",
    "x = Conv2Plus1D(filters=16, kernel_size=(3, 7, 7), padding='same')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.ReLU()(x)\n",
    "x = ResizeVideo(HEIGHT // 2, WIDTH // 2)(x)\n",
    "\n",
    "# Block 1\n",
    "x = add_residual_block(x, 16, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 4, WIDTH // 4)(x)\n",
    "\n",
    "# Block 2\n",
    "x = add_residual_block(x, 32, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 8, WIDTH // 8)(x)\n",
    "\n",
    "# Block 3\n",
    "x = add_residual_block(x, 64, (3, 3, 3))\n",
    "x = ResizeVideo(HEIGHT // 16, WIDTH // 16)(x)\n",
    "\n",
    "# Block 4\n",
    "x = add_residual_block(x, 128, (3, 3, 3))\n",
    "\n",
    "x = layers.GlobalAveragePooling3D()(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(10)(x)\n",
    "\n",
    "model = keras.Model(input, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TiO0WylG-ZHM"
   },
   "outputs": [],
   "source": [
    "frames, label = next(iter(train_ds))\n",
    "model.build(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GAsKrM8r-bKM"
   },
   "outputs": [],
   "source": [
    "# Visualize the model\n",
    "keras.utils.plot_model(model, expand_nested=True, dpi=60, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yvJJPnY-dMP"
   },
   "source": [
    "## Entrenar el modelo\n",
    "\n",
    "Para este tutorial, seleccione el optimizador `tf.keras.optimizers.Adam` y la función de pérdida `tf.keras.losses.SparseCategoricalCrossentropy`. Use el argumento `metrics` para ver la precisión del rendimiento del modelo en cada paso."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ejrbyebDp2tA"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.SparseCategoricalCrossentropy(from_logits=True), \n",
    "              optimizer = keras.optimizers.Adam(learning_rate = 0.0001), \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZT1Xlx9stP2"
   },
   "source": [
    "Entrene el modelo durante 50 épocas con el método `Model.fit` de Keras.\n",
    "\n",
    "Nota: Este modelo de ejemplo se entrena con menos puntos de datos (300 ejemplos de entrenamiento y 100 de validación) para mantener un tiempo de entrenamiento razonable para este tutorial. Además, este modelo de ejemplo puede tardar más de una hora en entrenarse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMrMUl2hOqMs"
   },
   "outputs": [],
   "source": [
    "history = model.fit(x = train_ds,\n",
    "                    epochs = 50, \n",
    "                    validation_data = val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKUfMNVns2hu"
   },
   "source": [
    "### Visualizar los resultados\n",
    "\n",
    "Cree gráficos de la pérdida y la precisión en los conjuntos de entrenamiento y validación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cd5tpNrtOrs7"
   },
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "  \"\"\"\n",
    "    Plotting training and validation learning curves.\n",
    "\n",
    "    Args:\n",
    "      history: model history with all the metric measures\n",
    "  \"\"\"\n",
    "  fig, (ax1, ax2) = plt.subplots(2)\n",
    "\n",
    "  fig.set_size_inches(18.5, 10.5)\n",
    "\n",
    "  # Plot loss\n",
    "  ax1.set_title('Loss')\n",
    "  ax1.plot(history.history['loss'], label = 'train')\n",
    "  ax1.plot(history.history['val_loss'], label = 'test')\n",
    "  ax1.set_ylabel('Loss')\n",
    "  \n",
    "  # Determine upper bound of y-axis\n",
    "  max_loss = max(history.history['loss'] + history.history['val_loss'])\n",
    "\n",
    "  ax1.set_ylim([0, np.ceil(max_loss)])\n",
    "  ax1.set_xlabel('Epoch')\n",
    "  ax1.legend(['Train', 'Validation']) \n",
    "\n",
    "  # Plot accuracy\n",
    "  ax2.set_title('Accuracy')\n",
    "  ax2.plot(history.history['accuracy'],  label = 'train')\n",
    "  ax2.plot(history.history['val_accuracy'], label = 'test')\n",
    "  ax2.set_ylabel('Accuracy')\n",
    "  ax2.set_ylim([0, 1])\n",
    "  ax2.set_xlabel('Epoch')\n",
    "  ax2.legend(['Train', 'Validation'])\n",
    "\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EJrGF0Sss8E0"
   },
   "source": [
    "## Evaluar el modelo\n",
    "\n",
    "Usar `Model.evaluate` de Keras para conocer la pérdida y la precisión en el conjunto de datos de prueba.\n",
    "\n",
    "Nota: El modelo de ejemplo de este tutorial usa un subgrupo del conjunto de datos UCF101 para mantener un tiempo de entrenamiento razonable. La precisión y la pérdida pueden mejorarse con un mayor ajuste de hiperparámetros o con más datos de entrenamiento. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hev0hMCxOtfy"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, return_dict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-F73GxD1-yc8"
   },
   "source": [
    "Para visualizar mejor el rendimiento del modelo, use una [matriz de confusión](https://www.tensorflow.org/api_docs/python/tf/math/confusion_matrix). La matriz de confusión le permite evaluar el rendimiento del modelo de clasificación más allá de la precisión. Para construir la matriz de confusión para este problema de clasificación multiclase, obtenga los valores reales en el conjunto de pruebas y los valores predichos. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw-6rG5V-0L-"
   },
   "outputs": [],
   "source": [
    "def get_actual_predicted_labels(dataset): \n",
    "  \"\"\"\n",
    "    Create a list of actual ground truth values and the predictions from the model.\n",
    "\n",
    "    Args:\n",
    "      dataset: An iterable data structure, such as a TensorFlow Dataset, with features and labels.\n",
    "\n",
    "    Return:\n",
    "      Ground truth and predicted values for a particular dataset.\n",
    "  \"\"\"\n",
    "  actual = [labels for _, labels in dataset.unbatch()]\n",
    "  predicted = model.predict(dataset)\n",
    "\n",
    "  actual = tf.stack(actual, axis=0)\n",
    "  predicted = tf.concat(predicted, axis=0)\n",
    "  predicted = tf.argmax(predicted, axis=1)\n",
    "\n",
    "  return actual, predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aln6qWW_-2dk"
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(actual, predicted, labels, ds_type):\n",
    "  cm = tf.math.confusion_matrix(actual, predicted)\n",
    "  ax = sns.heatmap(cm, annot=True, fmt='g')\n",
    "  sns.set(rc={'figure.figsize':(12, 12)})\n",
    "  sns.set(font_scale=1.4)\n",
    "  ax.set_title('Confusion matrix of action recognition for ' + ds_type)\n",
    "  ax.set_xlabel('Predicted Action')\n",
    "  ax.set_ylabel('Actual Action')\n",
    "  plt.xticks(rotation=90)\n",
    "  plt.yticks(rotation=0)\n",
    "  ax.xaxis.set_ticklabels(labels)\n",
    "  ax.yaxis.set_ticklabels(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tfQ3VAGd-4Az"
   },
   "outputs": [],
   "source": [
    "fg = FrameGenerator(subset_paths['train'], n_frames, training=True)\n",
    "labels = list(fg.class_ids_for_name.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ucGpbiA-5qi"
   },
   "outputs": [],
   "source": [
    "actual, predicted = get_actual_predicted_labels(train_ds)\n",
    "plot_confusion_matrix(actual, predicted, labels, 'training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mfr7AT5T-7ZD"
   },
   "outputs": [],
   "source": [
    "actual, predicted = get_actual_predicted_labels(test_ds)\n",
    "plot_confusion_matrix(actual, predicted, labels, 'test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FefzeIZz-9aI"
   },
   "source": [
    "Los valores de precisión y memoria de cada clase también pueden calcularse utilizando una matriz de confusión."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dq95-56Z-_E2"
   },
   "outputs": [],
   "source": [
    "def calculate_classification_metrics(y_actual, y_pred, labels):\n",
    "  \"\"\"\n",
    "    Calculate the precision and recall of a classification model using the ground truth and\n",
    "    predicted values. \n",
    "\n",
    "    Args:\n",
    "      y_actual: Ground truth labels.\n",
    "      y_pred: Predicted labels.\n",
    "      labels: List of classification labels.\n",
    "\n",
    "    Return:\n",
    "      Precision and recall measures.\n",
    "  \"\"\"\n",
    "  cm = tf.math.confusion_matrix(y_actual, y_pred)\n",
    "  tp = np.diag(cm) # Diagonal represents true positives\n",
    "  precision = dict()\n",
    "  recall = dict()\n",
    "  for i in range(len(labels)):\n",
    "    col = cm[:, i]\n",
    "    fp = np.sum(col) - tp[i] # Sum of column minus true positive is false negative\n",
    "    \n",
    "    row = cm[i, :]\n",
    "    fn = np.sum(row) - tp[i] # Sum of row minus true positive, is false negative\n",
    "    \n",
    "    precision[labels[i]] = tp[i] / (tp[i] + fp) # Precision \n",
    "    \n",
    "    recall[labels[i]] = tp[i] / (tp[i] + fn) # Recall\n",
    "  \n",
    "  return precision, recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jSEonYQ_BZt"
   },
   "outputs": [],
   "source": [
    "precision, recall = calculate_classification_metrics(actual, predicted, labels) # Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hXvTW1Df_DV8"
   },
   "outputs": [],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be1yrQl5_EYF"
   },
   "outputs": [],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4WsP4Z2HZ6L"
   },
   "source": [
    "## Siguientes pasos\n",
    "\n",
    "Para más información sobre cómo trabajar con datos de video en TensorFlow, consulte los siguientes tutoriales:\n",
    "\n",
    "- [Carga de datos de video](https://www.tensorflow.org/tutorials/load_data/video)\n",
    "- [MoViNet para reconocimiento de acciones de transmisión](https://www.tensorflow.org/hub/tutorials/movinet)\n",
    "- [Aprendizaje por transferencia para clasificación de video con MoViNet](https://www.tensorflow.org/tutorials/video/transfer_learning_with_movinet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cZCM65CBt1CJ"
   },
   "source": [
    "##### Copyright 2019 The TensorFlow Authors.\n",
    "\n",
    "Licensed under the Apache License, Version 2.0 (the \"License\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "JOgMcEajtkmg"
   },
   "outputs": [],
   "source": [
    "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "# https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rCSP-dbMw88x"
   },
   "source": [
    "# Segmentación de imágenes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEWs8JXRuGex"
   },
   "source": [
    "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
    "  <td><a target=\"_blank\" href=\"https://www.tensorflow.org/tutorials/images/segmentation\"><img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\">Ver en TensorFlow.org</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\">Ejecutar en Google Colab</a></td>\n",
    "  <td><a target=\"_blank\" href=\"https://github.com/tensorflow/docs-l10n/blob/master/site/es-419/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\">Ver fuente en GitHub</a></td>\n",
    "  <td><a href=\"https://storage.googleapis.com/tensorflow_docs/docs-l10n/site/es-419/tutorials/images/segmentation.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\">Descargar el bloc de notas</a></td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sMP7mglMuGT2"
   },
   "source": [
    "Este tutorial se enfoca en la tarea de segmentación de imágenes, usando una <a href=\"https://lmb.informatik.uni-freiburg.de/people/ronneber/u-net/\" class=\"external\">U-Net</a> modificada.\n",
    "\n",
    "## ¿Qué es la segmentación de imágenes?\n",
    "\n",
    "En una tarea de clasificación de imágenes, la red asigna una etiqueta (o clase) a cada imagen de entrada. Pero supongamos que desea conocer la forma de ese objeto, qué pixel pertenece a qué objeto, etc. En este caso, necesita asignar una clase a cada pixel de la imagen; esta tarea se conoce como segmentación. Un modelo de segmentación devuelve información mucho más detallada sobre la imagen. La segmentación de imágenes tiene muchas aplicaciones en el diagnóstico médico por imagen, los coches autónomos y las imágenes por satélite, por nombrar sólo algunas.\n",
    "\n",
    "Este tutorial usa el conjunto de datos [Oxford-IIIT Pet](https://www.robots.ox.ac.uk/~vgg/data/pets/) ([Parkhi et al, 2012](https://www.robots.ox.ac.uk/~vgg/publications/2012/parkhi12a/parkhi12a.pdf)). El conjunto de datos incluye imágenes de 37 razas de mascotas, con 200 imágenes por raza (~100 cada una en divisiones de entrenamiento y prueba). Cada imagen incluye las etiquetas correspondientes y máscaras a nivel de pixel. Las máscaras son etiquetas de clase para cada pixel. A cada pixel se le asigna una de tres categorías:\n",
    "\n",
    "- Clase 1: Pixel perteneciente a la mascota.\n",
    "- Clase 2: Pixel que bordea la mascota.\n",
    "- Clase 3: Ninguna de las anteriores/un pixel circundante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MQmKthrSBCld"
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/tensorflow/examples.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQX7R4bhZy5h"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g87--n2AtyO_"
   },
   "outputs": [],
   "source": [
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oWe0_rQM4JbC"
   },
   "source": [
    "## Descargar el conjunto de datos Oxford-IIIT Pets\n",
    "\n",
    "El conjunto de datos está [disponible en Conjuntos de datos TensorFlow](https://www.tensorflow.org/datasets/catalog/oxford_iiit_pet). Las máscaras de segmentación se incluyen a partir de la versión 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "40ITeStwDwZb"
   },
   "outputs": [],
   "source": [
    "dataset, info = tfds.load('oxford_iiit_pet:3.*.*', with_info=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rJcVdj_U4vzf"
   },
   "source": [
    "Además, los valores de color de la imagen se normalizan al rango `[0, 1]`. Por último, como se ha mencionado anteriormente, los pixeles de la máscara de segmentación se etiquetan como {1, 2, 3}. Para que resulte más cómodo, se resta 1 de la máscara de segmentación, lo que da como resultado las etiquetas: {0, 1, 2}."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FD60EbcAQqov"
   },
   "outputs": [],
   "source": [
    "def normalize(input_image, input_mask):\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  input_mask -= 1\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zf0S67hJRp3D"
   },
   "outputs": [],
   "source": [
    "def load_image(datapoint):\n",
    "  input_image = tf.image.resize(datapoint['image'], (128, 128))\n",
    "  input_mask = tf.image.resize(\n",
    "    datapoint['segmentation_mask'],\n",
    "    (128, 128),\n",
    "    method = tf.image.ResizeMethod.NEAREST_NEIGHBOR,\n",
    "  )\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "65-qHTjX5VZh"
   },
   "source": [
    "El conjunto de datos ya contiene las divisiones de entrenamiento y prueba necesarias, así que siga usando las mismas divisiones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yHwj2-8SaQli"
   },
   "outputs": [],
   "source": [
    "TRAIN_LENGTH = info.splits['train'].num_examples\n",
    "BATCH_SIZE = 64\n",
    "BUFFER_SIZE = 1000\n",
    "STEPS_PER_EPOCH = TRAIN_LENGTH // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "39fYScNz9lmo"
   },
   "outputs": [],
   "source": [
    "train_images = dataset['train'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "test_images = dataset['test'].map(load_image, num_parallel_calls=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9hGHyg8L3Y1"
   },
   "source": [
    "La siguiente clase ejecuta un simple aumento volteando aleatoriamente una imagen. Vaya al tutorial [Aumento de imágenes](data_augmentation.ipynb) para obtener más información.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fUWdDJRTL0PP"
   },
   "outputs": [],
   "source": [
    "class Augment(tf.keras.layers.Layer):\n",
    "  def __init__(self, seed=42):\n",
    "    super().__init__()\n",
    "    # both use the same seed, so they'll make the same random changes.\n",
    "    self.augment_inputs = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "    self.augment_labels = tf.keras.layers.RandomFlip(mode=\"horizontal\", seed=seed)\n",
    "  \n",
    "  def call(self, inputs, labels):\n",
    "    inputs = self.augment_inputs(inputs)\n",
    "    labels = self.augment_labels(labels)\n",
    "    return inputs, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTIbNIBdcgL3"
   },
   "source": [
    "Construya la canalización de entrada, aplicando el aumento tras la división en lotes de las entradas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VPscskQcNCx4"
   },
   "outputs": [],
   "source": [
    "train_batches = (\n",
    "    train_images\n",
    "    .cache()\n",
    "    .shuffle(BUFFER_SIZE)\n",
    "    .batch(BATCH_SIZE)\n",
    "    .repeat()\n",
    "    .map(Augment())\n",
    "    .prefetch(buffer_size=tf.data.AUTOTUNE))\n",
    "\n",
    "test_batches = test_images.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xa3gMAE_9qNa"
   },
   "source": [
    "Visualice un ejemplo de imagen y su máscara correspondiente desde el conjunto de datos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3N2RPAAW9q4W"
   },
   "outputs": [],
   "source": [
    "def display(display_list):\n",
    "  plt.figure(figsize=(15, 15))\n",
    "\n",
    "  title = ['Input Image', 'True Mask', 'Predicted Mask']\n",
    "\n",
    "  for i in range(len(display_list)):\n",
    "    plt.subplot(1, len(display_list), i+1)\n",
    "    plt.title(title[i])\n",
    "    plt.imshow(tf.keras.utils.array_to_img(display_list[i]))\n",
    "    plt.axis('off')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6u_Rblkteqb"
   },
   "outputs": [],
   "source": [
    "for images, masks in train_batches.take(2):\n",
    "  sample_image, sample_mask = images[0], masks[0]\n",
    "  display([sample_image, sample_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FAOe93FRMk3w"
   },
   "source": [
    "## Definir el modelo\n",
    "\n",
    "El modelo que se usa aquí es una [U-Net](https://arxiv.org/abs/1505.04597) modificada. Una U-Net consta de un codificador (downsampler) y un decodificador (upsampler). Si quiere aprender características robustas y reducir el número de parámetros entrenables, use un modelo preentrenado ([MobileNetV2](https://arxiv.org/abs/1801.04381)) como codificador. Para el decodificador, usará el bloque upsample, que ya está implementado en el ejemplo [pix2pix](https://github.com/tensorflow/examples/blob/master/tensorflow_examples/models/pix2pix/pix2pix.py) del repositorio de ejemplos de TensorFlow. (Eche un vistazo al tutorial [pix2pix: Traducción imagen a imagen con un GAN condicional](../generative/pix2pix.ipynb) en un bloc de notas).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W4mQle3lthit"
   },
   "source": [
    "Como se ha mencionado, el codificador es un modelo MobileNetV2 preentrenado. Usted usará el modelo de `tf.keras.applications`. El codificador consiste en salidas específicas de las capas intermedias del modelo. Tenga en cuenta que el codificador no se entrenará durante el proceso de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "liCeLH0ctjq7"
   },
   "outputs": [],
   "source": [
    "base_model = tf.keras.applications.MobileNetV2(input_shape=[128, 128, 3], include_top=False)\n",
    "\n",
    "# Use the activations of these layers\n",
    "layer_names = [\n",
    "    'block_1_expand_relu',   # 64x64\n",
    "    'block_3_expand_relu',   # 32x32\n",
    "    'block_6_expand_relu',   # 16x16\n",
    "    'block_13_expand_relu',  # 8x8\n",
    "    'block_16_project',      # 4x4\n",
    "]\n",
    "base_model_outputs = [base_model.get_layer(name).output for name in layer_names]\n",
    "\n",
    "# Create the feature extraction model\n",
    "down_stack = tf.keras.Model(inputs=base_model.input, outputs=base_model_outputs)\n",
    "\n",
    "down_stack.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KPw8Lzra5_T9"
   },
   "source": [
    "El decodificador/upsampler es simplemente una serie de bloques de upsample implementados en ejemplos de TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p0ZbfywEbZpJ"
   },
   "outputs": [],
   "source": [
    "up_stack = [\n",
    "    pix2pix.upsample(512, 3),  # 4x4 -> 8x8\n",
    "    pix2pix.upsample(256, 3),  # 8x8 -> 16x16\n",
    "    pix2pix.upsample(128, 3),  # 16x16 -> 32x32\n",
    "    pix2pix.upsample(64, 3),   # 32x32 -> 64x64\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "45HByxpVtrPF"
   },
   "outputs": [],
   "source": [
    "def unet_model(output_channels:int):\n",
    "  inputs = tf.keras.layers.Input(shape=[128, 128, 3])\n",
    "\n",
    "  # Downsampling through the model\n",
    "  skips = down_stack(inputs)\n",
    "  x = skips[-1]\n",
    "  skips = reversed(skips[:-1])\n",
    "\n",
    "  # Upsampling and establishing the skip connections\n",
    "  for up, skip in zip(up_stack, skips):\n",
    "    x = up(x)\n",
    "    concat = tf.keras.layers.Concatenate()\n",
    "    x = concat([x, skip])\n",
    "\n",
    "  # This is the last layer of the model\n",
    "  last = tf.keras.layers.Conv2DTranspose(\n",
    "      filters=output_channels, kernel_size=3, strides=2,\n",
    "      padding='same')  #64x64 -> 128x128\n",
    "\n",
    "  x = last(x)\n",
    "\n",
    "  return tf.keras.Model(inputs=inputs, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LRsjdZuEnZfA"
   },
   "source": [
    "Tenga en cuenta que el número de filtros de la última capa se establece en función del número de `output_channels`. Esto será un canal de salida por clase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j0DGH_4T0VYn"
   },
   "source": [
    "## Entrenar el modelo\n",
    "\n",
    "Ahora sólo queda compilar y entrenar el modelo.\n",
    "\n",
    "Dado que se trata de un problema de clasificación multiclase, use la función de pérdida `tf.keras.losses.SparseCategoricalCrossentropy` con el argumento `from_logits` como `True`, ya que las etiquetas son enteros escalares en lugar de vectores de puntuaciones para cada pixel de cada clase.\n",
    "\n",
    "Al ejecutar la inferencia, la etiqueta asignada al pixel es el canal con el valor más alto. Esto es lo que hace la función `create_mask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6he36HK5uKAc"
   },
   "outputs": [],
   "source": [
    "OUTPUT_CLASSES = 3\n",
    "\n",
    "model = unet_model(output_channels=OUTPUT_CLASSES)\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xVMzbIZLcyEF"
   },
   "source": [
    "Trace la arquitectura del modelo resultante:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sw82qF1Gcovr"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(model, show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tc3MiEO2twLS"
   },
   "source": [
    "Pruebe el modelo para comprobar lo que predice antes del entrenamiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UwvIKLZPtxV_"
   },
   "outputs": [],
   "source": [
    "def create_mask(pred_mask):\n",
    "  pred_mask = tf.math.argmax(pred_mask, axis=-1)\n",
    "  pred_mask = pred_mask[..., tf.newaxis]\n",
    "  return pred_mask[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YLNsrynNtx4d"
   },
   "outputs": [],
   "source": [
    "def show_predictions(dataset=None, num=1):\n",
    "  if dataset:\n",
    "    for image, mask in dataset.take(num):\n",
    "      pred_mask = model.predict(image)\n",
    "      display([image[0], mask[0], create_mask(pred_mask)])\n",
    "  else:\n",
    "    display([sample_image, sample_mask,\n",
    "             create_mask(model.predict(sample_image[tf.newaxis, ...]))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X_1CC0T4dho3"
   },
   "outputs": [],
   "source": [
    "show_predictions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22AyVYWQdkgk"
   },
   "source": [
    "La retrollamada definida a continuación se usa para observar cómo mejora el modelo mientras se entrena:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wHrHsqijdmL6"
   },
   "outputs": [],
   "source": [
    "class DisplayCallback(tf.keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    clear_output(wait=True)\n",
    "    show_predictions()\n",
    "    print ('\\nSample Prediction after epoch {}\\n'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "StKDH_B9t4SD"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 20\n",
    "VAL_SUBSPLITS = 5\n",
    "VALIDATION_STEPS = info.splits['test'].num_examples//BATCH_SIZE//VAL_SUBSPLITS\n",
    "\n",
    "model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                          steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                          validation_steps=VALIDATION_STEPS,\n",
    "                          validation_data=test_batches,\n",
    "                          callbacks=[DisplayCallback()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P_mu0SAbt40Q"
   },
   "outputs": [],
   "source": [
    "loss = model_history.history['loss']\n",
    "val_loss = model_history.history['val_loss']\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(model_history.epoch, loss, 'r', label='Training loss')\n",
    "plt.plot(model_history.epoch, val_loss, 'bo', label='Validation loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Value')\n",
    "plt.ylim([0, 1])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unP3cnxo_N72"
   },
   "source": [
    "## Hacer predicciones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7BVXldSo-0mW"
   },
   "source": [
    "Ahora, haga algunas predicciones. Para ahorrar tiempo, mantuvimos un número reducido de épocas, pero puede fijarlo más alto para obtener resultados más precisos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ikrzoG24qwf5"
   },
   "outputs": [],
   "source": [
    "show_predictions(test_batches, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QAwvlgSNoK3o"
   },
   "source": [
    "## Opcional: Clases desequilibradas y ponderaciones por clase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eqtFPqqu2kxP"
   },
   "source": [
    "Los conjuntos de datos de segmentación semántica pueden estar muy desequilibrados, lo que significa que los pixeles de una clase concreta pueden estar más presentes dentro de las imágenes que los de otras clases. Dado que los problemas de segmentación pueden tratarse como problemas de clasificación por pixel, puede abordar el problema del desequilibrio asignando ponderaciones a la función de pérdida para compensarlo. Es una forma sencilla y elegante de lidiar con este problema. Consulte el tutorial [Clasificación en datos desequilibrados](../structured_data/imbalanced_data.ipynb) para saber más.\n",
    "\n",
    "Para [evitar ambigüedades](https://github.com/keras-team/keras/issues/3653#issuecomment-243939748), `Model.fit` no admite el argumento `class_weight` para objetivos con más de 3 dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aHt90UEQsZDn"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  model_history = model.fit(train_batches, epochs=EPOCHS,\n",
    "                            steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                            class_weight = {0:2.0, 1:2.0, 2:1.0})\n",
    "  assert False\n",
    "except Exception as e:\n",
    "  print(f\"Expected {type(e).__name__}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brbhYODCsvbe"
   },
   "source": [
    "Por tanto, en este caso deberá implementar usted mismo la ponderación. Lo hará usando ponderaciones de muestreo: Además de los pares `(data, label)`, `Model.fit` también acepta los tripletes `(data, label, sample_weight)`.\n",
    "\n",
    "`Model.fit` de Keras propaga el `sample_weight` a las pérdidas y métricas, que también aceptan un argumento `sample_weight`. La ponderación de la muestra se multiplica por el valor de la muestra antes del paso de reducción. Por ejemplo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EmHtImJn5Kk-"
   },
   "outputs": [],
   "source": [
    "label = [0,0]\n",
    "prediction = [[-3., 0], [-3, 0]] \n",
    "sample_weight = [1, 10] \n",
    "\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True,\n",
    "                                               reduction=tf.keras.losses.Reduction.NONE)\n",
    "loss(label, prediction, sample_weight).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gbwo3DZ-9TxM"
   },
   "source": [
    "Así, para hacer ponderaciones de muestra para este tutorial, se necesita una función que tome un par `(data, label)` y devuelva un triplete `(data, label, sample_weight)` donde el `sample_weight` es una imagen de 1 canal que contiene la ponderación de clase para cada pixel.\n",
    "\n",
    "La implementación más sencilla posible es usar la etiqueta como índice en una lista `class_weight`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DlG-n2Ugo8Jc"
   },
   "outputs": [],
   "source": [
    "def add_sample_weights(image, label):\n",
    "  # The weights for each class, with the constraint that:\n",
    "  #     sum(class_weights) == 1.0\n",
    "  class_weights = tf.constant([2.0, 2.0, 1.0])\n",
    "  class_weights = class_weights/tf.reduce_sum(class_weights)\n",
    "\n",
    "  # Create an image of `sample_weights` by using the label at each pixel as an \n",
    "  # index into the `class weights` .\n",
    "  sample_weights = tf.gather(class_weights, indices=tf.cast(label, tf.int32))\n",
    "\n",
    "  return image, label, sample_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLH_NvH2UrXU"
   },
   "source": [
    "Los elementos del conjunto de datos resultante contienen 3 imágenes cada uno:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SE_ezRSFRCnE"
   },
   "outputs": [],
   "source": [
    "train_batches.map(add_sample_weights).element_spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yc-EpIzaRbSL"
   },
   "source": [
    "Ahora ya puede entrenar un modelo en este conjunto de datos ponderado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QDWipedAoOQe"
   },
   "outputs": [],
   "source": [
    "weighted_model = unet_model(OUTPUT_CLASSES)\n",
    "weighted_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "btEFKc1xodGR"
   },
   "outputs": [],
   "source": [
    "weighted_model.fit(\n",
    "    train_batches.map(add_sample_weights),\n",
    "    epochs=1,\n",
    "    steps_per_epoch=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R24tahEqmSCk"
   },
   "source": [
    "## Siguientes pasos\n",
    "\n",
    "Ahora que ya sabe qué es la segmentación de imágenes y cómo funciona, puede probar este tutorial con diferentes salidas de capas intermedias, o incluso con diferentes modelos preentrenados. También puede retarse a sí mismo probando el reto de enmascaramiento de imágenes de [Carvana](https://www.kaggle.com/c/carvana-image-masking-challenge/overview) alojado en Kaggle.\n",
    "\n",
    "También puede consultar la [API de detección de objetos de Tensorflow](https://github.com/tensorflow/models/blob/master/research/object_detection/README.md) para ver otro modelo que puede volver a entrenar con sus propios datos. Los modelos preentrenados están disponibles en [TensorFlow Hub](https://www.tensorflow.org/hub/tutorials/tf2_object_detection#optional)."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "multi_worker_with_keras.ipynb",
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
